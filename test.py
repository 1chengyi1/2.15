import streamlit as st
import pandas as pd
import networkx as nx
import numpy as np
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from zhipuai import ZhipuAI
import os

# è®¾ç½®æ™ºè°± API å¯†é’¥
client = ZhipuAI(api_key="89c41de3c3a34f62972bc75683c66c72.ZGwzmpwgMfjtmksz")

# ==========================
# æ•°æ®é¢„å¤„ç†æ¨¡å—
# ==========================
@st.cache_data(show_spinner=False)
def process_risk_data():
    # ä¸ç«¯åŸå› ä¸¥é‡æ€§æƒé‡
    misconduct_weights = {
        'ä¼ªé€ ã€ç¯¡æ”¹å›¾ç‰‡': 6,
        'ç¯¡æ”¹å›¾ç‰‡': 3,
        'ç¯¡æ”¹æ•°æ®': 3,
        'ç¯¡æ”¹æ•°æ®ã€å›¾ç‰‡': 6,
        'ç¼–é€ ç ”ç©¶è¿‡ç¨‹': 4,
        'ç¼–é€ ç ”ç©¶è¿‡ç¨‹ã€ä¸å½“ç½²å': 7,
        'ç¯¡æ”¹æ•°æ®ã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±': 2,
        'å®éªŒæµç¨‹ä¸è§„èŒƒ': 2,
        'æ•°æ®å®¡æ ¸ä¸ä¸¥': 2,
        'ç½²åä¸å½“ã€å®éªŒæµç¨‹ä¸è§„èŒƒ': 5,
        'ç¯¡æ”¹æ•°æ®ã€ä»£å†™ä»£æŠ•ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ä¸å½“ç½²å': 13,
        'ç¯¡æ”¹æ•°æ®ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ä¸å½“ç½²å': 8,
        'ç¬¬ä¸‰æ–¹ä»£å†™ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±': 7,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ä¼ªé€ æ•°æ®': 8,
        'ä¸€ç¨¿å¤šæŠ•': 2,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ä¼ªé€ æ•°æ®ã€ä¸€ç¨¿å¤šæŠ•': 10,
        'ç¯¡æ”¹æ•°æ®ã€å‰½çªƒ': 8,
        'ä¼ªé€ å›¾ç‰‡': 3,
        'ä¼ªé€ å›¾ç‰‡ã€ä¸å½“ç½²å': 6,
        'å§”æ‰˜å®éªŒã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ æ•°æ®': 3,
        'ä¼ªé€ æ•°æ®ã€ç¯¡æ”¹å›¾ç‰‡': 6,
        'ä¼ªé€ æ•°æ®ã€ä¸å½“ç½²åã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ç­‰': 8,
        'ä¼ªé€ æ•°æ®ã€ä¸€å›¾å¤šç”¨ã€ä¼ªé€ å›¾ç‰‡ã€ä»£æŠ•é—®é¢˜': 14,
        'ä¼ªé€ æ•°æ®ã€ç½²åä¸å½“': 6,
        'æŠ„è¢­å‰½çªƒä»–äººé¡¹ç›®ç”³è¯·ä¹¦å†…å®¹': 6,
        'ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ç¯¡æ”¹æ•°æ®å’Œå›¾ç‰‡': 8,
        'ç¯¡æ”¹æ•°æ®ã€ä¸å½“ç½²å': 6,
        'æŠ„è¢­ä»–äººåŸºé‡‘é¡¹ç›®ç”³è¯·ä¹¦': 6,
        'ç»“é¢˜æŠ¥å‘Šä¸­å­˜åœ¨è™šå‡ä¿¡æ¯': 5,
        'æŠ„è¢­å‰½çªƒ': 5,
        'é€ å‡ã€æŠ„è¢­': 5,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•': 5,
        'ç½²åä¸å½“': 3,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ç½²åä¸å½“': 8,
        'æŠ„è¢­å‰½çªƒã€ä¼ªé€ æ•°æ®': 8,
        'ä¹°å–å›¾ç‰‡æ•°æ®': 3,
        'ä¹°å–æ•°æ®': 3,
        'ä¹°å–è®ºæ–‡': 5,
        'ä¹°å–è®ºæ–‡ã€ä¸å½“ç½²å': 8,
        'ä¹°å–è®ºæ–‡æ•°æ®': 8,
        'ä¹°å–è®ºæ–‡æ•°æ®ã€ä¸å½“ç½²å': 11,
        'ä¹°å–å›¾ç‰‡æ•°æ®ã€ä¸å½“ç½²å': 6,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€ä¼ªé€ æ•°æ®': 6,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€æ•°æ®é€ å‡ã€æœªç»åŒæ„ä½¿ç”¨ä»–äººç½²å': 9,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€æ•°æ®é€ å‡ã€æœªç»åŒæ„ä½¿ç”¨ä»–äººç½²åã€ç¼–é€ ç ”ç©¶è¿‡ç¨‹': 13,
        'å›¾ç‰‡é€ å‡ã€ä¸å½“ç½²å': 9,
        'å›¾ç‰‡é€ å‡ã€ä¸å½“ç½²åã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ç­‰': 11,
        'ä¹°å–æ•°æ®ã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ è®ºæ–‡ã€ä¸å½“ç½²å': 6,
        'å…¶ä»–è½»å¾®ä¸ç«¯è¡Œä¸º': 1
    }

    # è¯»å–æ•°æ®å¹¶æ„å»ºç½‘ç»œï¼ˆä¿æŒåŸæœ‰ç½‘ç»œæ„å»ºé€»è¾‘ä¸å˜ï¼‰
    # è¯»å–åŸå§‹æ•°æ®
    papers_df = pd.read_excel('å®éªŒæ•°æ®.xlsx', sheet_name='è®ºæ–‡')
    projects_df = pd.read_excel('å®éªŒæ•°æ®.xlsx', sheet_name='é¡¹ç›®')

    # ======================
    # ç½‘ç»œæ„å»ºå‡½æ•°
    # ======================
    def build_networks(papers, projects):
        # ä½œè€…-è®ºæ–‡ç½‘ç»œ
        G_papers = nx.Graph()
        for _, row in papers.iterrows():
            authors = [row['å§“å']]
            weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
            G_papers.add_edge(row['å§“å'], row['ä¸ç«¯å†…å®¹'], weight=weight)

        # ä½œè€…-é¡¹ç›®ç½‘ç»œ
        G_projects = nx.Graph()
        for _, row in projects.iterrows():
            authors = [row['å§“å']]
            weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
            G_projects.add_edge(row['å§“å'], row['ä¸ç«¯å†…å®¹'], weight=weight)

        # ä½œè€…-ä½œè€…ç½‘ç»œ
        G_authors = nx.Graph()

        # å…±åŒé¡¹ç›®/è®ºæ–‡è¿æ¥
        for df in [papers, projects]:
            for _, row in df.iterrows():
                authors = [row['å§“å']]
                weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
                for i in range(len(authors)):
                    for j in range(i + 1, len(authors)):
                        if G_authors.has_edge(authors[i], authors[j]):
                            G_authors[authors[i]][authors[j]]['weight'] += weight
                        else:
                            G_authors.add_edge(authors[i], authors[j], weight=weight)

        # ç ”ç©¶æ–¹å‘ç›¸ä¼¼æ€§è¿æ¥
        research_areas = papers.groupby('å§“å')['ç ”ç©¶æ–¹å‘'].apply(lambda x: ' '.join(x)).reset_index()
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(research_areas['ç ”ç©¶æ–¹å‘'])
        similarity_matrix = cosine_similarity(tfidf_matrix)

        for i in range(len(research_areas)):
            for j in range(i + 1, len(research_areas)):
                if similarity_matrix[i, j] > 0.7:
                    a1 = research_areas.iloc[i]['å§“å']
                    a2 = research_areas.iloc[j]['å§“å']
                    G_authors.add_edge(a1, a2, weight=similarity_matrix[i, j], reason='ç ”ç©¶æ–¹å‘ç›¸ä¼¼')

        # å…±åŒæœºæ„è¿æ¥
        institution_map = papers.set_index('å§“å')['ç ”ç©¶æœºæ„'].to_dict()
        for a1 in institution_map:
            for a2 in institution_map:
                if a1 != a2 and institution_map[a1] == institution_map[a2]:
                    G_authors.add_edge(a1, a2, weight=1, reason='ç ”ç©¶æœºæ„ç›¸åŒ')

        return G_authors

    # ======================
    # Word2Vecï¼ˆSkip-gramï¼‰æ¨¡å‹å®šä¹‰
    # ======================
    class SkipGramModel(nn.Module):
        def __init__(self, vocab_size, embedding_size):
            super(SkipGramModel, self).__init__()
            self.embeddings = nn.Embedding(vocab_size, embedding_size)
            self.out = nn.Linear(embedding_size, vocab_size)

        def forward(self, inputs):
            embeds = self.embeddings(inputs)
            outputs = self.out(embeds)
            return outputs

    # ======================
    # æ•°æ®é›†å®šä¹‰
    # ======================
    class SkipGramDataset(Dataset):
        def __init__(self, walks, node2id):
            self.walks = walks
            self.node2id = node2id

        def __len__(self):
            return len(self.walks)

        def __getitem__(self, idx):
            walk = self.walks[idx]
            input_ids = [self.node2id[node] for node in walk[:-1]]
            target_ids = [self.node2id[node] for node in walk[1:]]
            return torch.tensor(input_ids), torch.tensor(target_ids)

    # ======================
    # DeepWalkå®ç°
    # ======================
    def deepwalk(graph, walk_length=30, num_walks=200, embedding_size=128):
        walks = []
        nodes = list(graph.nodes())

        for _ in range(num_walks):
            random.shuffle(nodes)
            for node in nodes:
                walk = [str(node)]
                current = node
                for _ in range(walk_length - 1):
                    neighbors = list(graph.neighbors(current))
                    if neighbors:
                        current = random.choice(neighbors)
                        walk.append(str(current))
                    else:
                        break
                walks.append(walk)

        # æ„å»ºèŠ‚ç‚¹åˆ°IDçš„æ˜ å°„
        node2id = {node: idx for idx, node in enumerate(set([node for walk in walks for node in walk]))}
        id2node = {idx: node for node, idx in node2id.items()}

        # æ„å»ºæ•°æ®é›†
        dataset = SkipGramDataset(walks, node2id)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

        # æ¨¡å‹åˆå§‹åŒ–
        model = SkipGramModel(len(node2id), embedding_size)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        # è®­ç»ƒæ¨¡å‹
        for epoch in range(10):
            for inputs, targets in dataloader:
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs.view(-1, len(node2id)), targets.view(-1))
                loss.backward()
                optimizer.step()

        # è·å–åµŒå…¥
        embeddings = {}
        with torch.no_grad():
            for node, idx in node2id.items():
                embeddings[node] = model.embeddings(torch.tensor([idx])).squeeze().numpy()

        return embeddings

    # ======================
    # æ‰§è¡Œè®¡ç®—æµç¨‹
    # ======================
    with st.spinner('æ­£åœ¨æ„å»ºåˆä½œç½‘ç»œ...'):
        G_authors = build_networks(papers_df, projects_df)

    with st.spinner('æ­£åœ¨è®­ç»ƒDeepWalkæ¨¡å‹...'):
        embeddings = deepwalk(G_authors)

    with st.spinner('æ­£åœ¨è®¡ç®—é£é™©æŒ‡æ ‡...'):
        # æ„å»ºåˆ†ç±»æ•°æ®é›†
        X, y = [], []
        for edge in G_authors.edges():
            X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
            y.append(1)

        non_edges = list(nx.non_edges(G_authors))
        non_edges = random.sample(non_edges, len(y))
        for edge in non_edges:
            X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
            y.append(0)

        # è®­ç»ƒåˆ†ç±»å™¨
        X = np.array(X)
        y = np.array(y)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        clf = RandomForestClassifier(n_estimators=100)
        clf.fit(X_train, y_train)

        # è®¡ç®—èŠ‚ç‚¹é£é™©å€¼
        risk_scores = {node: np.linalg.norm(emb) for node, emb in embeddings.items()}

    return pd.DataFrame({
        'ä½œè€…': list(risk_scores.keys()),
        'é£é™©å€¼': list(risk_scores.values())
    }), papers_df, projects_df

# ==========================
# æ™ºè°±å¤§æ¨¡å‹äº¤äº’æ¨¡å—
# ==========================
def get_zhipu_evaluation(selected, paper_records, project_records):
    """è·å–åŒ…å«ç½‘ç»œæœç´¢çš„æ·±åº¦åˆ†ææŠ¥å‘Š"""
    prompt_template = f"""
è¯·ä¸ºç§‘ç ”äººå‘˜ã€{selected}ã€‘ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šï¼Œéœ€åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

ä¸€ã€å­¦æœ¯èƒŒæ™¯åˆ†æï¼ˆåŸºäºç½‘ç»œå…¬å¼€ä¿¡æ¯ï¼‰
1. æ•™è‚²ç»å†ï¼šæ¯•ä¸šé™¢æ ¡ã€å­¦ä½ä¿¡æ¯
2. ä»»èŒæœºæ„ï¼šå½“å‰åŠå†å²ä»»èŒæƒ…å†µ
3. ç ”ç©¶æ–¹å‘ï¼šä¸»è¦ç ”ç©¶é¢†åŸŸåŠç»†åˆ†æ–¹å‘
4. å­¦æœ¯æˆæœï¼šä»£è¡¨æ€§è®ºæ–‡ã€ä¸“åˆ©ã€é¡¹ç›®ï¼ˆåˆ—ä¸¾3-5ä¸ªé‡ç‚¹æˆæœï¼‰

äºŒã€ç§‘ç ”è¯šä¿¡è¯„ä¼°ï¼ˆç»“åˆå›½å®¶æ”¿ç­–ï¼‰
æ ¹æ®ä»¥ä¸‹æ”¿ç­–åˆ†æå†å²è®°å½•ï¼š
- ã€Šç§‘ç ”è¯šä¿¡æ¡ˆä»¶è°ƒæŸ¥å¤„ç†è§„åˆ™ï¼ˆè¯•è¡Œï¼‰ã€‹
- ã€Šå…³äºè¿›ä¸€æ­¥åŠ å¼ºç§‘ç ”è¯šä¿¡å»ºè®¾çš„è‹¥å¹²æ„è§ã€‹
- ã€Šç§‘å­¦æŠ€æœ¯æ´»åŠ¨è¿è§„è¡Œä¸ºå¤„ç†æš‚è¡Œè§„å®šã€‹
è¯„ä¼°ç»´åº¦ï¼š
1. è¡Œä¸ºä¸¥é‡æ€§åˆ†æ
2. æ•´æ”¹æƒ…å†µè¿½è¸ª
3. æ½œåœ¨å½±å“è¯„ä¼°

ä¸‰ã€åˆä½œç½‘ç»œåˆ†æï¼ˆåŸºäºå…¬å¼€æ•°æ®ï¼‰
1. é«˜é¢‘åˆä½œè€…ï¼ˆåˆ—å‡º5-10äººï¼‰
2. åˆä½œå½¢å¼åˆ†æï¼ˆè®ºæ–‡/é¡¹ç›®/ä¸“åˆ©ç­‰ï¼‰
3. æœºæ„å…³è”ç½‘ç»œ
4. å›½é™…åˆä½œæƒ…å†µ

å››ã€é£é™©é¢„è­¦å»ºè®®
1. ç›‘ç®¡å…³æ³¨å»ºè®®
2. åˆä½œé£é™©æç¤º
3. é¡¹ç›®è¯„å®¡å»ºè®®

æ ¼å¼è¦æ±‚ï¼š
## å­¦æœ¯èƒŒæ™¯
...
## è¯šä¿¡è¯„ä¼°  
...
## åˆä½œç½‘ç»œ
...
## é£é™©é¢„è­¦
..."""

    try:
        response = client.chat.completions.create(
            model="glm-4v-plus",
            messages=[{
                "role": "user",
                "content": prompt_template,
                "temperature": 0.3
            }]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"## æœåŠ¡å¼‚å¸¸\n{str(e)}"

# ==========================
# å¯è§†åŒ–ç•Œé¢æ¨¡å—
# ==========================
def main():
    st.set_page_config(
        page_title="ç§‘ç ”è¯šä¿¡æ™ºèƒ½åˆ†æå¹³å°",
        page_icon="ğŸ”¬",
        layout="wide"
    )

    # è‡ªå®šä¹‰æ ·å¼
    st.markdown("""
    <style>
    .report-section { 
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 15px 0;
        background: white;
    }
    .section-title {
        color: #1e3d6d;
        border-left: 4px solid #1e3d6d;
        padding-left: 10px;
    }
    .risk-alert {
        color: #d32f2f;
        background: #ffebee;
        padding: 15px;
        border-radius: 8px;
    }
    </style>
    """, unsafe_allow_html=True)

    # ä¾§è¾¹æ ï¼ˆä¿æŒåŸæœ‰æ•°æ®åŠ è½½é€»è¾‘ä¸å˜ï¼‰
    # ...

    # ä¸»ç•Œé¢
    st.title("ğŸ” ç§‘ç ”äººå‘˜æ·±åº¦åˆ†æç³»ç»Ÿ")
    
    # æœç´¢åŠŸèƒ½ï¼ˆä¿æŒåŸæœ‰æœç´¢é€»è¾‘ä¸å˜ï¼‰
    search_term = st.text_input("è¾“å…¥ç ”ç©¶äººå‘˜å§“åï¼š", placeholder="æ”¯æŒä¸­è‹±æ–‡å§“åæœç´¢...")
    
    if search_term:
        # ...ï¼ˆä¿æŒåŸæœ‰æ•°æ®åŒ¹é…é€»è¾‘ï¼‰
        
        # ======================
        # æ™ºèƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆ
        # ======================
        if st.button(f"ğŸ•µï¸ ç”Ÿæˆ{selected}çš„æ™ºèƒ½åˆ†ææŠ¥å‘Š"):
            with st.spinner("æ­£åœ¨é€šè¿‡å­¦æœ¯å¤§æ•°æ®ç”Ÿæˆæ·±åº¦åˆ†æ..."):
                try:
                    report = get_zhipu_evaluation(selected, paper_records, project_records)
                    
                    # ç»“æ„åŒ–æ˜¾ç¤ºæŠ¥å‘Š
                    sections = {
                        "## å­¦æœ¯èƒŒæ™¯": "academic",
                        "## è¯šä¿¡è¯„ä¼°": "integrity",
                        "## åˆä½œç½‘ç»œ": "collab",
                        "## é£é™©é¢„è­¦": "risk"
                    }
                    
                    current_section = None
                    content_buffer = []
                    
                    for line in report.split('\n'):
                        line = line.strip()
                        if line in sections:
                            if current_section:
                                # è¾“å‡ºç¼“å†²å†…å®¹
                                with st.container():
                                    st.markdown(f'<div class="report-section" id="{sections[current_section]}">', unsafe_allow_html=True)
                                    st.markdown(f'<div class="section-title">{current_section}</div>', unsafe_allow_html=True)
                                    st.markdown('\n'.join(content_buffer))
                                    st.markdown('</div>', unsafe_allow_html=True)
                            current_section = line
                            content_buffer = []
                        else:
                            content_buffer.append(line)
                    
                    # å¤„ç†æœ€åä¸€ä¸ªéƒ¨åˆ†
                    if current_section:
                        with st.container():
                            st.markdown(f'<div class="report-section" id="{sections[current_section]}">', unsafe_allow_html=True)
                            st.markdown(f'<div class="section-title">{current_section}</div>', unsafe_allow_html=True)
                            st.markdown('\n'.join(content_buffer))
                            st.markdown('</div>', unsafe_allow_html=True)
                            
                            # ç‰¹åˆ«æ ‡æ³¨é£é™©é¢„è­¦
                            if "é£é™©é¢„è­¦" in current_section:
                                st.markdown('<div class="risk-alert">âš ï¸ è¯·é‡ç‚¹å…³æ³¨é£é™©é¢„è­¦å†…å®¹</div>', unsafe_allow_html=True)
                                
                except Exception as e:
                    st.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{str(e)}")

        # ...ï¼ˆä¿æŒåŸæœ‰çš„ç½‘ç»œå¯è§†åŒ–ç­‰ç»„ä»¶ï¼‰

if __name__ == "__main__":
    main()
