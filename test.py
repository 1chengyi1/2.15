import streamlit as st
import pandas as pd
import networkx as nx
import numpy as np
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score
import plotly.graph_objects as go
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import zhipuai

# è®¾ç½®æ™ºè°±æ¸…è¨€ API å¯†é’¥
zhipuai.api_key = "89c41de3c3a34f62972bc75683c66c72.ZGwzmpwgMfjtmksz"  # è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API å¯†é’¥

# ==========================
# æ•°æ®é¢„å¤„ç†å’Œé£é™©å€¼è®¡ç®—æ¨¡å—
# ==========================
@st.cache_data(show_spinner=False)
def process_risk_data():
    # ä¸ç«¯åŸå› ä¸¥é‡æ€§æƒé‡
    misconduct_weights = {
        'ä¼ªé€ ã€ç¯¡æ”¹å›¾ç‰‡': 6,
        'ç¯¡æ”¹å›¾ç‰‡': 3,
        'ç¯¡æ”¹æ•°æ®': 3,
        'ç¯¡æ”¹æ•°æ®ã€å›¾ç‰‡': 6,
        'ç¼–é€ ç ”ç©¶è¿‡ç¨‹': 4,
        'ç¼–é€ ç ”ç©¶è¿‡ç¨‹ã€ä¸å½“ç½²å': 7,
        'ç¯¡æ”¹æ•°æ®ã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±': 2,
        'å®éªŒæµç¨‹ä¸è§„èŒƒ': 2,
        'æ•°æ®å®¡æ ¸ä¸ä¸¥': 2,
        'ç½²åä¸å½“ã€å®éªŒæµç¨‹ä¸è§„èŒƒ': 5,
        'ç¯¡æ”¹æ•°æ®ã€ä»£å†™ä»£æŠ•ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ä¸å½“ç½²å': 13,
        'ç¯¡æ”¹æ•°æ®ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ä¸å½“ç½²å': 8,
        'ç¬¬ä¸‰æ–¹ä»£å†™ã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±': 7,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ä¼ªé€ æ•°æ®': 8,
        'ä¸€ç¨¿å¤šæŠ•': 2,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ä¼ªé€ æ•°æ®ã€ä¸€ç¨¿å¤šæŠ•': 10,
        'ç¯¡æ”¹æ•°æ®ã€å‰½çªƒ': 8,
        'ä¼ªé€ å›¾ç‰‡': 3,
        'ä¼ªé€ å›¾ç‰‡ã€ä¸å½“ç½²å': 6,
        'å§”æ‰˜å®éªŒã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ æ•°æ®': 3,
        'ä¼ªé€ æ•°æ®ã€ç¯¡æ”¹å›¾ç‰‡': 6,
        'ä¼ªé€ æ•°æ®ã€ä¸å½“ç½²åã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ç­‰': 8,
        'ä¼ªé€ æ•°æ®ã€ä¸€å›¾å¤šç”¨ã€ä¼ªé€ å›¾ç‰‡ã€ä»£æŠ•é—®é¢˜': 14,
        'ä¼ªé€ æ•°æ®ã€ç½²åä¸å½“': 6,
        'æŠ„è¢­å‰½çªƒä»–äººé¡¹ç›®ç”³è¯·ä¹¦å†…å®¹': 6,
        'ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ã€ç¯¡æ”¹æ•°æ®å’Œå›¾ç‰‡': 8,
        'ç¯¡æ”¹æ•°æ®ã€ä¸å½“ç½²å': 6,
        'æŠ„è¢­ä»–äººåŸºé‡‘é¡¹ç›®ç”³è¯·ä¹¦': 6,
        'ç»“é¢˜æŠ¥å‘Šä¸­å­˜åœ¨è™šå‡ä¿¡æ¯': 5,
        'æŠ„è¢­å‰½çªƒ': 5,
        'é€ å‡ã€æŠ„è¢­': 5,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•': 5,
        'ç½²åä¸å½“': 3,
        'ç¬¬ä¸‰æ–¹ä»£å†™ä»£æŠ•ã€ç½²åä¸å½“': 8,
        'æŠ„è¢­å‰½çªƒã€ä¼ªé€ æ•°æ®': 8,
        'ä¹°å–å›¾ç‰‡æ•°æ®': 3,
        'ä¹°å–æ•°æ®': 3,
        'ä¹°å–è®ºæ–‡': 5,
        'ä¹°å–è®ºæ–‡ã€ä¸å½“ç½²å': 8,
        'ä¹°å–è®ºæ–‡æ•°æ®': 8,
        'ä¹°å–è®ºæ–‡æ•°æ®ã€ä¸å½“ç½²å': 11,
        'ä¹°å–å›¾ç‰‡æ•°æ®ã€ä¸å½“ç½²å': 6,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€ä¼ªé€ æ•°æ®': 6,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€æ•°æ®é€ å‡ã€æœªç»åŒæ„ä½¿ç”¨ä»–äººç½²å': 9,
        'å›¾ç‰‡ä¸å½“ä½¿ç”¨ã€æ•°æ®é€ å‡ã€æœªç»åŒæ„ä½¿ç”¨ä»–äººç½²åã€ç¼–é€ ç ”ç©¶è¿‡ç¨‹': 13,
        'å›¾ç‰‡é€ å‡ã€ä¸å½“ç½²å': 9,
        'å›¾ç‰‡é€ å‡ã€ä¸å½“ç½²åã€ä¼ªé€ é€šè®¯ä½œè€…é‚®ç®±ç­‰': 11,
        'ä¹°å–æ•°æ®ã€ä¸å½“ç½²å': 6,
        'ä¼ªé€ è®ºæ–‡ã€ä¸å½“ç½²å': 6,
        'å…¶ä»–è½»å¾®ä¸ç«¯è¡Œä¸º': 1
    }

    # è¯»å–åŸå§‹æ•°æ®
    papers_df = pd.read_excel('data3.xlsx', sheet_name='è®ºæ–‡')
    projects_df = pd.read_excel('data3.xlsx', sheet_name='é¡¹ç›®')

    # ======================
    # ç½‘ç»œæ„å»ºå‡½æ•°
    # ======================
    def build_networks(papers, projects):
        # ä½œè€…-è®ºæ–‡ç½‘ç»œ
        G_papers = nx.Graph()
        for _, row in papers.iterrows():
            authors = [row['å§“å']]
            weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
            G_papers.add_edge(row['å§“å'], row['ä¸ç«¯å†…å®¹'], weight=weight)

        # ä½œè€…-é¡¹ç›®ç½‘ç»œ
        G_projects = nx.Graph()
        for _, row in projects.iterrows():
            authors = [row['å§“å']]
            weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
            G_projects.add_edge(row['å§“å'], row['ä¸ç«¯å†…å®¹'], weight=weight)

        # ä½œè€…-ä½œè€…ç½‘ç»œ
        G_authors = nx.Graph()

        # å…±åŒé¡¹ç›®/è®ºæ–‡è¿æ¥
        for df in [papers, projects]:
            for _, row in df.iterrows():
                authors = [row['å§“å']]
                weight = misconduct_weights.get(row['ä¸ç«¯åŸå› '], 1)
                for i in range(len(authors)):
                    for j in range(i + 1, len(authors)):
                        if G_authors.has_edge(authors[i], authors[j]):
                            G_authors[authors[i]][authors[j]]['weight'] += weight
                        else:
                            G_authors.add_edge(authors[i], authors[j], weight=weight)

        # ç ”ç©¶æ–¹å‘ç›¸ä¼¼æ€§è¿æ¥
        research_areas = papers.groupby('å§“å')['ç ”ç©¶æ–¹å‘'].apply(lambda x: ' '.join(x)).reset_index()
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(research_areas['ç ”ç©¶æ–¹å‘'])
        similarity_matrix = cosine_similarity(tfidf_matrix)

        for i in range(len(research_areas)):
            for j in range(i + 1, len(research_areas)):
                if similarity_matrix[i, j] > 0.7:
                    a1 = research_areas.iloc[i]['å§“å']
                    a2 = research_areas.iloc[j]['å§“å']
                    G_authors.add_edge(a1, a2, weight=similarity_matrix[i, j], reason='ç ”ç©¶æ–¹å‘ç›¸ä¼¼')

        # å…±åŒæœºæ„è¿æ¥
        institution_map = papers.set_index('å§“å')['ç ”ç©¶æœºæ„'].to_dict()
        for a1 in institution_map:
            for a2 in institution_map:
                if a1 != a2 and institution_map[a1] == institution_map[a2]:
                    G_authors.add_edge(a1, a2, weight=1, reason='ç ”ç©¶æœºæ„ç›¸åŒ')

        return G_authors

    # ======================
    # Word2Vecï¼ˆSkip-gramï¼‰æ¨¡å‹å®šä¹‰
    # ======================
    class SkipGramModel(nn.Module):
        def __init__(self, vocab_size, embedding_size):
            super(SkipGramModel, self).__init__()
            self.embeddings = nn.Embedding(vocab_size, embedding_size)
            self.out = nn.Linear(embedding_size, vocab_size)

        def forward(self, inputs):
            embeds = self.embeddings(inputs)
            outputs = self.out(embeds)
            return outputs

    # ======================
    # æ•°æ®é›†å®šä¹‰
    # ======================
    class SkipGramDataset(Dataset):
        def __init__(self, walks, node2id):
            self.walks = walks
            self.node2id = node2id

        def __len__(self):
            return len(self.walks)

        def __getitem__(self, idx):
            walk = self.walks[idx]
            input_ids = [self.node2id[node] for node in walk[:-1]]
            target_ids = [self.node2id[node] for node in walk[1:]]
            return torch.tensor(input_ids), torch.tensor(target_ids)

    # ======================
    # DeepWalkå®ç°
    # ======================
    def deepwalk(graph, walk_length=30, num_walks=200, embedding_size=128):
        walks = []
        nodes = list(graph.nodes())

        for _ in range(num_walks):
            random.shuffle(nodes)
            for node in nodes:
                walk = [str(node)]
                current = node
                for _ in range(walk_length - 1):
                    neighbors = list(graph.neighbors(current))
                    if neighbors:
                        current = random.choice(neighbors)
                        walk.append(str(current))
                    else:
                        break
                walks.append(walk)

        # æ„å»ºèŠ‚ç‚¹åˆ°IDçš„æ˜ å°„
        node2id = {node: idx for idx, node in enumerate(set([node for walk in walks for node in walk]))}
        id2node = {idx: node for node, idx in node2id.items()}

        # æ„å»ºæ•°æ®é›†
        dataset = SkipGramDataset(walks, node2id)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

        # æ¨¡å‹åˆå§‹åŒ–
        model = SkipGramModel(len(node2id), embedding_size)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        # è®­ç»ƒæ¨¡å‹
        for epoch in range(10):
            for inputs, targets in dataloader:
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs.view(-1, len(node2id)), targets.view(-1))
                loss.backward()
                optimizer.step()

        # è·å–åµŒå…¥
        embeddings = {}
        with torch.no_grad():
            for node, idx in node2id.items():
                embeddings[node] = model.embeddings(torch.tensor([idx])).squeeze().numpy()

        return embeddings

    # ======================
    # æ‰§è¡Œè®¡ç®—æµç¨‹
    # ======================
    with st.spinner('æ­£åœ¨æ„å»ºåˆä½œç½‘ç»œ...'):
        G_authors = build_networks(papers_df, projects_df)

    with st.spinner('æ­£åœ¨è®­ç»ƒDeepWalkæ¨¡å‹...'):
        embeddings = deepwalk(G_authors)

    with st.spinner('æ­£åœ¨è®¡ç®—é£é™©æŒ‡æ ‡...'):
        # æ„å»ºåˆ†ç±»æ•°æ®é›†
        X, y = [], []
        for edge in G_authors.edges():
            X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
            y.append(1)

        non_edges = list(nx.non_edges(G_authors))
        non_edges = random.sample(non_edges, len(y))
        for edge in non_edges:
            X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
            y.append(0)

        # è®­ç»ƒåˆ†ç±»å™¨
        X = np.array(X)
        y = np.array(y)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        clf = RandomForestClassifier(n_estimators=100)
        clf.fit(X_train, y_train)

        # è®¡ç®—èŠ‚ç‚¹é£é™©å€¼
        risk_scores = {node: np.linalg.norm(emb) for node, emb in embeddings.items()}

    return pd.DataFrame({
        'ä½œè€…': list(risk_scores.keys()),
        'é£é™©å€¼': list(risk_scores.values())
    }), papers_df, projects_df

# è°ƒç”¨æ™ºè°±æ¸…è¨€ API ç”Ÿæˆç®€å†å’Œè¯„ä»·
def generate_resume_and_evaluation(author, paper_records, project_records, risk_value):
    prompt = f"è¯·ä¸ºç§‘ç ”äººå‘˜ {author} ç”Ÿæˆä¸€ä»½ç®€å†å’Œè¯„ä»·ã€‚è¯¥ç§‘ç ”äººå‘˜çš„è®ºæ–‡ä¸ç«¯è®°å½•å¦‚ä¸‹ï¼š{paper_records.to_csv(sep='\t', na_rep='nan')}ï¼Œé¡¹ç›®ä¸ç«¯è®°å½•å¦‚ä¸‹ï¼š{project_records.to_csv(sep='\t', na_rep='nan')}ï¼Œä¿¡ç”¨é£é™©å€¼ä¸º {risk_value}ã€‚"
    response = zhipuai.model_api.invoke(
        model="chatglm_turbo",
        prompt=[{"role": "user", "content": prompt}]
    )
    if response['code'] == 200:
        return response['data']['choices'][0]['content']
    else:
        return f"è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š{response['code']}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{response['msg']}"

# ==========================
# å¯è§†åŒ–ç•Œé¢æ¨¡å—
# ==========================
def main():
    st.set_page_config(
        page_title="ç§‘ç ”äººå‘˜è¯šä¿¡é£é™©é¢„è­¦å¹³å°",
        page_icon="ğŸ”¬",
        layout="wide"
    )

    # è‡ªå®šä¹‰CSSæ ·å¼
    st.markdown("""
    <style>
.high - risk { color: red; font - weight: bold; animation: blink 1s infinite; }
    @keyframes blink { 0% {opacity:1;} 50% {opacity:0;} 100% {opacity:1;} }
.metric - box { padding: 20px; border - radius: 10px; background: #f0f2f6; margin: 10px; }
    table {
        table - layout: fixed;
    }
    table td {
        white - space: normal;
    }
 .stDataFrame tbody tr {
        display: block;
        overflow - y: auto;
        height: 200px;
    }
 .stDataFrame tbody {
        display: block;
    }
    </style>
    """, unsafe_allow_html=True)

    # ä¾§è¾¹æ æ§åˆ¶é¢æ¿ä¸Šæ–¹æ·»åŠ æ™ºè°±æ¸…è¨€å¤§æ¨¡å‹æŒ‰é’®
    if st.sidebar.button("ğŸ§  æ™ºè°±æ¸…è¨€ç”Ÿæˆç®€å†å’Œè¯„ä»·", help="æŸ¥æ‰¾ç§‘ç ”äººå‘˜åç‚¹å‡»æ­¤æŒ‰é’®ç”Ÿæˆç®€å†å’Œè¯„ä»·"):
        if 'selected_author' in st.session_state:
            author = st.session_state.selected_author
            author_risk = st.session_state.author_risk
            paper_records = st.session_state.paper_records
            project_records = st.session_state.project_records
            with st.spinner("æ­£åœ¨è°ƒç”¨æ™ºè°±æ¸…è¨€ç”Ÿæˆç®€å†å’Œè¯„ä»·..."):
                result = generate_resume_and_evaluation(author, paper_records, project_records, author_risk)
            st.subheader("ğŸ“‹ æ™ºè°±æ¸…è¨€ç”Ÿæˆçš„ç®€å†å’Œè¯„ä»·")
            st.write(result)
        else:
            st.warning("è¯·å…ˆæœç´¢å¹¶é€‰æ‹©ä¸€ä¸ªç§‘ç ”äººå‘˜")

    # ä¾§è¾¹æ æ§åˆ¶é¢æ¿
    with st.sidebar:
        st.title("æ§åˆ¶é¢æ¿")
        if st.button("ğŸ”„ é‡æ–°è®¡ç®—é£é™©å€¼", help="å½“åŸå§‹æ•°æ®æ›´æ–°åç‚¹å‡»æ­¤æŒ‰é’®"):
            with st.spinner("é‡æ–°è®¡ç®—ä¸­..."):
                risk_df, papers, projects = process_risk_data()
                risk_df.to_excel('risk_scores.xlsx', index=False)
            st.success("é£é™©å€¼æ›´æ–°å®Œæˆï¼")

        # æ·»åŠ â€œè¿”å›é¦–é¡µâ€æŒ‰é’®
        if st.button("ğŸ  è¿”å›é¦–é¡µ", help="ç‚¹å‡»è¿”å›é¦–é¡µ"):
            st.markdown("[ç‚¹å‡»è¿™é‡Œè¿”å›é¦–é¡µ](https://chengyi10.wordpress.com/)", unsafe_allow_html=True)

    # å°è¯•åŠ è½½ç°æœ‰æ•°æ®
    try:
        risk_df = pd.read_excel('risk_scores.xlsx')
        papers = pd.read_excel('data3.xlsx', sheet_name='è®ºæ–‡')
        projects = pd.read_excel('data3.xlsx', sheet_name='é¡¹ç›®')
    except:
        with st.spinner("é¦–æ¬¡è¿è¡Œéœ€è¦åˆå§‹åŒ–æ•°æ®..."):
            risk_df, papers, projects = process_risk_data()
            risk_df.to_excel('risk_scores.xlsx', index=False)

    # ä¸»ç•Œé¢
    st.title("ğŸ” ç§‘ç ”äººå‘˜ä¿¡ç”¨é£é™©é¢„è­¦ç³»ç»Ÿ")

    # æœç´¢æ¡†
    search_term = st.text_input("è¾“å…¥ç ”ç©¶äººå‘˜å§“åï¼š", placeholder="æ”¯æŒæ¨¡ç³Šæœç´¢...")

    if search_term:
        # æ¨¡ç³ŠåŒ¹é…
        candidates = risk_df[risk_df['ä½œè€…'].str.contains(search_term)]
        if len(candidates) == 0:
            st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„ç ”ç©¶äººå‘˜")
            return

        # ç›´æ¥é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…äººå‘˜
        selected = candidates['ä½œè€…'].iloc[0]

        # è·å–è¯¦ç»†ä¿¡æ¯
        author_risk = risk_df[risk_df['ä½œè€…'] == selected].iloc[0]['é£é™©å€¼']
        paper_records = papers[papers['å§“å'] == selected]
        project_records = projects[projects['å§“å'] == selected]

        # ä¿å­˜é€‰ä¸­çš„ç§‘ç ”äººå‘˜ä¿¡æ¯åˆ° session_state
        st.session_state.selected_author = selected
        st.session_state.author_risk = author_risk
        st.session_state.paper_records = paper_records
        st.session_state.project_records = project_records

        # ======================
        # ä¿¡æ¯å±•ç¤º
        # ======================
        st.subheader("ğŸ“„ è®ºæ–‡è®°å½•")
        if not paper_records.empty:
            # æ·»åŠ ç«–å‘æ»šåŠ¨æ¡
            st.markdown(
                """
                <style>
                .scrollable-table {
                    max-height: 300px;  /* è®¾ç½®æœ€å¤§é«˜åº¦ */
                    overflow-y: auto;   /* æ·»åŠ ç«–å‘æ»šåŠ¨æ¡ */
                    display: block;
                }
                </style>
                """,
                unsafe_allow_html=True
            )
            # å°† DataFrame è½¬æ¢ä¸º HTMLï¼Œå¹¶æ·»åŠ æ»šåŠ¨æ¡æ ·å¼
            st.markdown(
                f'<div class="scrollable-table">{paper_records.to_html(escape=False, index=False)}</div>',
                unsafe_allow_html=True
            )
        else:
            st.info("æš‚æ— è®ºæ–‡ä¸ç«¯è®°å½•")

        st.subheader("ğŸ“‹ é¡¹ç›®è®°å½•")
        if not project_records.empty:
            st.markdown(project_records.to_html(escape=False), unsafe_allow_html=True)
        else:
            st.info("æš‚æ— é¡¹ç›®ä¸ç«¯è®°å½•")

        # é£é™©æŒ‡æ ‡
        st.subheader("ğŸ“Š é£é™©åˆ†æ")
        risk_level = "high" if author_risk > 12 else "low"
        cols = st.columns(4)
        cols[0].metric("ä¿¡ç”¨é£é™©å€¼", f"{author_risk:.2f}",
                       delta_color="inverse" if risk_level == "high" else "normal")
        cols[1].metric("é£é™©ç­‰çº§",
                       f"{'âš ï¸ é«˜é£é™©' if risk_level == 'high' else 'âœ… ä½é£é™©'}",
                       help="é«˜é£é™©é˜ˆå€¼ï¼š12")

        # ======================
        # å…³ç³»ç½‘ç»œå¯è§†åŒ–
        # ======================
        with st.expander("ğŸ•¸ï¸ å±•å¼€åˆä½œå…³ç³»ç½‘ç»œ", expanded=True):
            def build_network_graph(author):
                G = nx.Graph()
                G.add_node(author)

                # æŸ¥æ‰¾ä¸æŸ¥è¯¢ä½œè€…æœ‰å…±åŒç ”ç©¶æœºæ„ã€ç ”ç©¶æ–¹å‘æˆ–ä¸ç«¯å†…å®¹çš„ä½œè€…
                related = papers[
                    (papers['ç ”ç©¶æœºæ„'] == papers[papers['å§“å'] == author]['ç ”ç©¶æœºæ„'].iloc[0]) |
                    (papers['ç ”ç©¶æ–¹å‘'] == papers[papers['å§“å'] == author]['ç ”ç©¶æ–¹å‘'].iloc[0]) |
                    (papers['ä¸ç«¯å†…å®¹'] == papers[papers['å§“å'] == author]['ä¸ç«¯å†…å®¹'].iloc[0])
                ]['å§“å'].unique()

                for person in related:
                    if person != author:
                        reason = ''
                        if papers[(papers['å§“å'] == author) & (papers['ç ”ç©¶æœºæ„'] == papers[papers['å§“å'] == person]['ç ”ç©¶æœºæ„'].iloc[0])].shape[0] > 0:
                            reason = 'ç ”ç©¶æœºæ„ç›¸åŒ'
                        elif papers[(papers['å§“å'] == author) & (papers['ç ”ç©¶æ–¹å‘'] == papers[papers['å§“å'] == person]['ç ”ç©¶æ–¹å‘'].iloc[0])].shape[0] > 0:
                            reason = 'ç ”ç©¶æ–¹å‘ç›¸ä¼¼'
                        else:
                            reason = 'ä¸ç«¯å†…å®¹ç›¸å…³'
                        G.add_node(person)
                        G.add_edge(author, person, label=reason)

                # ä½¿ç”¨ plotly ç»˜åˆ¶ç½‘ç»œå›¾
                pos = nx.spring_layout(G, k=0.5)  # å¸ƒå±€
                edge_trace = []
                edge_annotations = []  # ç”¨äºå­˜å‚¨è¾¹çš„æ ‡æ³¨ä¿¡æ¯
                for edge in G.edges(data=True):
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    edge_trace.append(go.Scatter(
                        x=[x0, x1, None], y=[y0, y1, None],
                        line=dict(width=0.5, color='#888'),
                        hoverinfo='text',
                        mode='lines'
                    ))

                    # è®¡ç®—è¾¹çš„ä¸­ç‚¹ä½ç½®ï¼Œç”¨äºæ”¾ç½®æ ‡æ³¨æ–‡å­—
                    mid_x = (x0 + x1) / 2
                    mid_y = (y0 + y1) / 2
                    edge_annotations.append(
                        dict(
                            x=mid_x,
                            y=mid_y,
                            xref='x',
                            yref='y',
                            text=edge[2]['label'],  # ç›¸è¿çš„åŸå› ä½œä¸ºæ ‡æ³¨æ–‡å­—
                            showarrow=False,
                            font=dict(size=10, color='black')
                        )
                    )

                node_trace = go.Scatter(
                    x=[], y=[], text=[], mode='markers+text', hoverinfo='text',
                    marker=dict(
                        showscale=True,
                        colorscale='YlGnBu',
                        size=10,
                    )
                )
                for node in G.nodes():
                    x, y = pos[node]
                    node_trace['x'] += tuple([x])
                    node_trace['y'] += tuple([y])
                    node_trace['text'] += tuple([node])

                fig = go.Figure(
                    data=edge_trace + [node_trace],
                    layout=go.Layout(
                        title='<br>åˆä½œå…³ç³»ç½‘ç»œå›¾',
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20, l=5, r=5, t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        annotations=edge_annotations  # æ·»åŠ è¾¹çš„æ ‡æ³¨ä¿¡æ¯
                    )
                )
                st.plotly_chart(fig, use_container_width=True)

            build_network_graph(selected)


if __name__ == "__main__":
    main()
